#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
5Unsupervised_Stroke_Detection.py

This script implements an unsupervised change‐point detection method to detect
tennis stroke boundaries from normalized pose estimation CSV data.

It performs the following steps:
1. Load the normalized CSV data (generated by 3Full_Video_Normalization.py).
2. Compute a composite velocity signal from selected key landmarks,
   with more emphasis (higher weight) on arm movement (elbows and wrists).
3. Compute a composite acceleration signal (as the derivative of velocity).
4. Smooth the acceleration signal with a Savitzky–Golay filter.
5. Run an unsupervised change‑point detection (using ruptures’ PELT) on the smoothed signal.
6. Convert the detected change points into stroke segmentation boundaries.
7. Use FFmpeg to clip the original video into stroke segments.

Notes:
- Minimum stroke duration is now set to 6 seconds.
- Adjust parameters as needed.
"""

import os
import csv
import math
import numpy as np
import cv2
import ffmpeg
import ruptures as rpt  # Unsupervised change-point detection
from scipy.signal import savgol_filter

#############################################
# CONFIGURATION SETTINGS
#############################################
# Paths (adjust as needed)
NORMALIZED_CSV = "Test_media/test_videos/video_1_normalized.csv"  # Normalized pose CSV file
ORIGINAL_VIDEO = "Test_media/test_videos/video_1.mp4"         # Original video file
OUTPUT_STROKE_FOLDER = "tennis_clips/strokes_unsupervised"    # Folder to save stroke clips
os.makedirs(OUTPUT_STROKE_FOLDER, exist_ok=True)

# Video parameters
FPS = 30  # Frames per second of the video

# Landmarks used to compute the composite metric:
# These are indices for: shoulders (11,12), elbows (13,14), wrists (15,16), and hips (23,24)
SELECTED_LANDMARKS = [11, 12, 13, 14, 15, 16, 23, 24]

# Define weights for each selected landmark
# More weight is given to the arm landmarks: elbows (13,14) and wrists (15,16)
LANDMARK_WEIGHTS = {lm: 2.0 if lm in [13, 14, 15, 16] else 1.0 for lm in SELECTED_LANDMARKS}

# Smoothing parameters for the acceleration signal
SMOOTH_WINDOW = 61      # Must be odd; adjust as needed
SMOOTH_POLY_ORDER = 2

# Parameters for change-point detection using ruptures (PELT method with 'rbf' cost)
RUPTURES_PENALTY = 10.0  # Adjust this penalty to change sensitivity

# Minimum stroke duration in seconds (updated to 6 seconds)
MIN_STROKE_DURATION_SEC = 6

#############################################
# FUNCTIONS
#############################################
def load_normalized_csv(csv_path):
    """
    Load the normalized CSV data.
    Assumes the CSV columns: frame_index, then 33*4 columns of raw pose data.
    Returns a list of rows (each row is a list of floats).
    """
    data = []
    with open(csv_path, "r") as f:
        reader = csv.reader(f)
        next(reader)  # skip header
        for row in reader:
            # Convert only the first (1+33*4=133) columns to float
            data.append([float(x) for x in row[:133]])
    return data

def extract_landmark_xy(frame_row, lm_index):
    """
    Given a frame row (list of floats) from the CSV,
    extract normalized x and y for landmark `lm_index`.
    The CSV columns: frame_index, lm_0_x, lm_0_y, lm_0_z, lm_0_vis, lm_1_x, ...
    So for landmark i, x is at index 1 + i*4 and y is at index 1 + i*4 + 1.
    """
    base = 1 + lm_index * 4
    x = frame_row[base]
    y = frame_row[base + 1]
    return x, y

def compute_composite_velocity(data):
    """
    Compute a composite velocity signal for each frame (except the first)
    based on the selected landmarks, with weighted emphasis on arm movement.
    For each frame, compute the Euclidean distance for each selected landmark from
    the previous frame, multiply by its weight, and compute the weighted average.
    Returns a NumPy array of velocity values.
    """
    velocities = []
    for i in range(1, len(data)):
        weighted_diffs = []
        total_weight = 0.0
        for lm in SELECTED_LANDMARKS:
            x1, y1 = extract_landmark_xy(data[i-1], lm)
            x2, y2 = extract_landmark_xy(data[i], lm)
            diff = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)
            weight = LANDMARK_WEIGHTS[lm]
            weighted_diffs.append(diff * weight)
            total_weight += weight
        velocities.append(np.sum(weighted_diffs) / total_weight)
    return np.array(velocities)

def compute_composite_acceleration(velocities, fps):
    """
    Compute composite acceleration as the numerical derivative of the velocity signal,
    multiplied by fps to scale per second.
    Returns a NumPy array of acceleration values.
    """
    acceleration = np.diff(velocities) * fps
    return acceleration

def smooth_signal(signal, window, poly_order):
    """
    Apply Savitzky–Golay filter to smooth the signal.
    """
    if len(signal) < window:
        window = len(signal) if len(signal) % 2 == 1 else len(signal) - 1
    return savgol_filter(signal, window_length=window, polyorder=poly_order)

def detect_change_points(signal, penalty):
    """
    Use the ruptures PELT algorithm with an 'rbf' cost to detect change points in the signal.
    Returns an array of indices in the signal where change points occur.
    """
    algo = rpt.Pelt(model="rbf").fit(signal)
    change_points = algo.predict(pen=penalty)
    # Remove the last change point if it is the end of the signal
    if change_points and change_points[-1] == len(signal):
        change_points = change_points[:-1]
    return np.array(change_points)

def clip_video(video_path, stroke_boundaries, fps, min_duration_sec, margin_sec=0):
    """
    Given the original video and a list of stroke boundary frame indices,
    compute stroke segments and clip the video using ffmpeg.
    Stroke segments are defined as the time between the midpoints of consecutive boundaries.
    Only segments longer than min_duration_sec are saved.
    """
    if len(stroke_boundaries) < 2:
        print("Not enough boundaries detected to clip strokes.")
        return

    # Convert frame indices to seconds
    times = [idx / fps for idx in stroke_boundaries]
    # Compute midpoints between consecutive boundaries
    stroke_cut_times = [(times[i] + times[i+1]) / 2 for i in range(len(times)-1)]
    
    # Define stroke segments as [start, end] pairs (adding optional margin)
    stroke_segments = []
    for i in range(len(stroke_cut_times)-1):
        t_start = max(0, stroke_cut_times[i] - margin_sec)
        t_end = stroke_cut_times[i+1] + margin_sec
        duration = t_end - t_start
        if duration < min_duration_sec:
            print(f"Skipping stroke segment {i+1} (duration {duration:.2f}s too short)")
            continue
        stroke_segments.append((t_start, t_end))
    
    # Clip each stroke segment using ffmpeg
    for i, (t_start, t_end) in enumerate(stroke_segments):
        output_clip = os.path.join(OUTPUT_STROKE_FOLDER, f"stroke_{i+1}.mp4")
        print(f"Clipping stroke {i+1}: start={t_start:.2f}s, end={t_end:.2f}s, duration={t_end - t_start:.2f}s")
        try:
            (
                ffmpeg
                .input(video_path, ss=t_start, to=t_end)
                .output(output_clip, codec="copy", loglevel="error", y=None)
                .run()
            )
        except Exception as e:
            print(f"Error clipping stroke {i+1}: {e}")
    print("Stroke clipping complete.")

#############################################
# MAIN EXECUTION
#############################################
def main():
    # Step 1: Load normalized CSV data
    print("Loading normalized CSV data...")
    data = load_normalized_csv(NORMALIZED_CSV)
    if not data:
        print("No data loaded from CSV!")
        return

    # Step 2: Compute composite velocity signal (with arm emphasis)
    print("Computing composite velocity...")
    comp_velocity = compute_composite_velocity(data)

    # Step 3: Compute composite acceleration signal
    print("Computing composite acceleration...")
    comp_acceleration = compute_composite_acceleration(comp_velocity, FPS)

    # Step 4: Smooth the acceleration signal
    print("Smoothing acceleration signal...")
    smooth_accel = smooth_signal(comp_acceleration, SMOOTH_WINDOW, SMOOTH_POLY_ORDER)

    # Step 5: Detect change points (unsupervised stroke boundaries)
    print("Detecting change points using PELT...")
    change_points = detect_change_points(smooth_accel, RUPTURES_PENALTY)
    # Map the change points back to original frame indices (adjust offset as needed)
    stroke_boundaries = (change_points + 1).tolist()
    print(f"Detected {len(stroke_boundaries)} stroke boundaries at frames: {stroke_boundaries}")

    # Step 6: Clip the video using the detected stroke boundaries
    print("Clipping strokes from video...")
    clip_video(ORIGINAL_VIDEO, stroke_boundaries, FPS, MIN_STROKE_DURATION_SEC, margin_sec=0)

if __name__ == '__main__':
    main()
